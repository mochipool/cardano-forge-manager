# Testnet Preprod Relay Node - Out-of-the-Box Example
# This is a complete, ready-to-deploy relay configuration for Cardano Preprod Testnet
# Optimized for development and testing with fast Mithril sync

# ===========================================
# BASIC CONFIGURATION
# ===========================================
replicaCount: 1

# ===========================================
# CARDANO NODE CONFIGURATION
# ===========================================
cardanoNode:
  network: "preprod"
  magic: "1"
  
  # Relay node configuration
  blockProducer: false
  
  # Enable Mithril for fast sync (essential for testnet)
  mithril:
    enabled: true
    restoreSnapshot: true
    aggregatorEndpoint: "https://aggregator.release-preprod.api.mithril.network/aggregator"
    genesisVerificationKey: "5b3132372c37332c3132342c3136312c362c3133372c3133312c3231332c3230372c3131372c3139382c38352c3137362c3139392c3136322c3234312c36382c3132332c3131392c3134352c31332c3233322c3234332c34392c3232392c322c3234392c3230352c3230352c33392c3233352c34345d"
    ancillaryVerificationKey: "5b3138392c3139322c3231362c3135302c3131342c3231362c3233372c3231302c34352c31382c32312c3139362c3230382c3234362c3134362c322c3235322c3234332c3235312c3139372c32382c3135372c3230342c3134352c33302c31342c3232382c3136382c3132392c38332c3133362c33365d"
  # Override requirements for preprod testnet
  config:
    requiresNetworkMagic: "RequiresMagic"

  # Preprod network topology
  topology:
    bootstrapPeers:
      - address: "preprod-node.play.dev.cardano.org"
        port: 3001
    localRoots:
      - accessPoints:
          - address: "cardano-bp-cardano-node-p2p.cardano-preprod.svc.cluster.local"  # Block Producer Internal DNS Name
            port: 3001
            description: "block-producer-1"
        advertise: false
        trustable: true
        hotValency: 2
    publicRoots:
      - accessPoints: []
        advertise: false
    useLedgerAfterSlot: -1

# ===========================================
# FORGE MANAGER V2.0
# ===========================================
forgeManager:
  enabled: false
  
# ===========================================
# STORAGE CONFIGURATION
# ===========================================
persistence:
  enabled: true
  size: 35Gi  # Preprod network requires more storage than preview
  # storageClass: ""  # Use default storage class

# ===========================================
# RESOURCE CONFIGURATION
# ===========================================
resources:
  cardanoNode:
    limits:
      cpu: 2000m
      memory: 6Gi
    requests:
      cpu: 1000m
      memory: 4Gi
  
  forgeManager:
    limits:
      cpu: 200m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi

# ===========================================
# NETWORKING
# ===========================================
service:
  p2p:
    type: LoadBalancer  # External access for relay
    port: 3001

# ===========================================
# MONITORING
# ===========================================
monitoring:
  enabled: true
  serviceMonitor:
    enabled: false  # Disable by default for simplicity

# ===========================================
# POD CONFIGURATION
# ===========================================
podLabels:
  network: "preprod"
  environment: "development"
  node-type: "relay"

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "12798"
  prometheus.io/path: "/metrics"

# ===========================================
# SECURITY CONTEXT
# ===========================================
securityContext:
  runAsNonRoot: true
  runAsUser: 10001
  runAsGroup: 10001

podSecurityContext:
  fsGroup: 10001
  runAsNonRoot: true
  runAsUser: 10001

# ===========================================
# DEPLOYMENT INSTRUCTIONS
# ===========================================
# 
# This example provides a complete testnet preprod relay deployment.
# Follow these steps to deploy:
#
# 1. Create namespace:
#    kubectl create namespace cardano-preprod
#
# 2. Deploy the relay:
#    helm install cardano-relay ./charts/cardano-node \
#      --namespace cardano-preprod \
#      --values ./charts/cardano-node/examples/testnet-preprod-relay.yaml
#
# 3. Monitor the deployment:
#    kubectl logs -f cardano-relay-0 -c cardano-node -n cardano-preprod
#    # You should see Mithril snapshot restoration messages
#
# ===========================================
# VERIFICATION STEPS
# ===========================================
#
# After deployment, verify everything is working:
#
# 1. Check node sync status:
#    kubectl exec -it cardano-relay-0 -c cardano-node -n cardano-preprod -- \
#      cardano-cli query tip --testnet-magic 1 --socket-path /ipc/node.socket
#
# 2. Check that the node socket exists:
#    kubectl exec -it cardano-relay-0 -c cardano-node -n cardano-preprod -- \
#      ls -la /ipc/node.socket
#
# 3. Test network connectivity:
#    kubectl exec -it cardano-relay-0 -c cardano-node -n cardano-preprod -- \
#      nc -zv preprod-node.play.dev.cardano.org 3001
#
# 4. Get external IP for connecting block producers:
#    kubectl get svc cardano-relay-cardano-node-p2p -n cardano-preprod
#
# ===========================================
# CUSTOMIZATION
# ===========================================
#
# Common customizations:
#
# 1. Adjust resources:
#    - Modify resources.cardanoNode.* for different CPU/memory allocation
#    - Modify persistence.size for different storage requirements
#
# 2. Add relay connections:
#    - Add entries to cardanoNode.topology.localRoots for private relays/block-producers
#
# 3. Change service type:
#    - Use NodePort instead of LoadBalancer if preferred
#    - Use ClusterIP for internal-only relays
#
# ===========================================
# EXPECTED BEHAVIOR
# ===========================================
#
# 1. Node will start and restore from Mithril snapshot (much faster than full sync)
# 2. External LoadBalancer service will be created for P2P connections
# 3. Metrics will be available on port 12798 (cardano-node)
# 4. Node will sync with preprod network and accept connections
#
# ===========================================
# TROUBLESHOOTING
# ===========================================
#
# Common issues and solutions:
#
# 1. Pod stuck in pending:
#    Check if the storage class exists and has available capacity
#
# 2. Node not syncing:
#    Check network connectivity and Mithril configuration
#
# 3. LoadBalancer stuck pending:
#    Verify your cluster supports LoadBalancer services (cloud provider required)
#
# 4. "requiresNetworkMagic" errors:
#    Ensure config.requiresNetworkMagic is set to "RequiresMagic" for testnets